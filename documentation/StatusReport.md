Since our initial status report, we had to make some changes when it came to our original project. We ran into an issue with our original dataset which related to United States traffic fatalities. We found that we didn’t have the licensing to download that said dataset. We had to make a pivot and find a new dataset that we were still interested in. We decided to find a new dataset that is called US Traffic Accidents. This dataset contains information regarding traffic accidents that happened in the United States disregarding Alaska and Hawaii. When we stumbled upon this dataset, we found this to be quite great as it has more in-depth information on each accident including whether alcohol was involved in the accident. With this switch however, our timeline got a little delayed, which we will update later in this report. Our original research topic stayed the same even with the change as we will be still looking at alcohol consumption and accidents. Since we do not have the fatality metric, we will be looking at other metrics instead such as severity of the crashes and frequency based on location. With both of us seeing firsthand the effects of alcohol on others, especially at a university level, we wanted to take a deep dive into how it affects driving on a national scale.
	 To start this project, we needed to create a structure and organize our data so that it could not only be reproducible in the future, but so that we could have a clear structure that allowed for ease of use between the two of us. The first step that we took was creating a file structure that allowed for the separation of all the different tasks that we worked on. This meant having folders for our raw data that we got from Kaggle, a scripts folder where we placed all the scripts we made for cleaning and grabbing data, processed data, and our documentation. We decided that to keep things clear, we would need to make sure that our pushes to the repository were clear in what happened. So, this includes adding a concise description on what was changed such as “created file structure for our repository” allows for us to follow along with each other and the work we are doing. Along with this, we had code reviews together in person where we would look at the work each person was doing so that we could provide suggestions and feedback to each other. Another step we made was to add comments and descriptive variable names so that each of us knew what exactly each line of code was doing.
	Since we got delayed a little bit with some unforeseen circumstances, we have an updated timeline. Since we were able to build out our file structure and write descriptions on how to access the data as well as the initial combining and cleaning of our data, we can move onto the exploration analysis next. Within the next week, we will perform basic exploration analysis to get a basis on what the data is holding. After that, we will work on making predictive models. We will then create scripts to automate the processes of acquisition, cleaning, and modeling so that it can be reproduced in the future. We will then work to finalize the markdown reports in which we will look at the FAIR principles giving a description of our ratings. We will finally end it with a final check and release of our project before the final project due date.
