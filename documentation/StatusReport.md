## Status Report
	
	Since our initial status report, we had to make some changes when it came to our original project. We ran into an issue with our original dataset which related to United States traffic fatalities. We found that we didn’t have the licensing to download that said dataset. We had to make a pivot and find a new dataset that we were still interested in. We decided to find a new dataset that is called State Crime US. The State Crime dataset gives detailed crime statistics for every U.S. state from 1960 to 2019. It was a compiled dataset based on data collected through the FBI’s Uniform Crime Reporting (UCR) program. It includes property crimes with different categories and violent crimes with different categories. For each state and year, the dataset reports the total number of crimes in each category as well as the crime rates per 100,000 residents. With this switch however, our timeline got a little delayed, which we will update later in this report. Our original research topic was adjusted with the change that was made, we will be still looking at alcohol consumption but now with correlation to crime in the US. Since our target metrics have changed, we will be looking at other metrics such as severity of violent crimes and frequency based on states. With both of us seeing firsthand the effects of alcohol on others, especially at a university level, we wanted to take a deep dive into how it affects crime on a national scale.
	 To start this project, we needed to create a structure and organize our data so that it could not only be reproducible in the future, but so that we could have a clear structure that allowed for ease of use between the two of us. The first step that we took was creating a file structure that allowed for the separation of all the different tasks that we worked on. This meant having folders for our raw data that we got from Kaggle, a scripts folder where we placed all the scripts we made for cleaning and grabbing data, processed data, and our documentation. We decided that to keep things clear, we would need to make sure that our pushes to the repository were clear in what happened. So, this includes adding a concise description on what was changed such as “created file structure for our repository” allows for us to follow along with each other and the work we are doing. Along with this, we had code reviews together in person where we would look at the work each person was doing so that we could provide suggestions and feedback to each other. Another step we made was to add comments and descriptive variable names so that each of us knew what exactly each line of code was doing.
	We have touched on some of the data exploration with the progress we have made on this project. We have started a jupiter notebook for each of the datasets that we have to look at the shape of our data as well as datatypes, to see if there are compatible types. We have also started some data visualization just to get a good big-picture idea on what the data entails. We used heat-maps and histograms initially to see what the distribution looks like. 
	Another prominent step that we have made is we have started preprocessing the data with initial cleaning so that it becomes compatible for integration. For this we used open refine to make some manual changes that were necessary for the data to be more organized and more compatible with each other. The first step I did was change the column names of the crime dataset. The column names were formatted in a non-organized and consistent manner so I changed their formatting for usability purposes. I also transformed both of the year columns in both the datasets to be numerical instead of text values. By doing this I was able to make the next step of getting rid of rows of data that had years that were not present in the other dataset. After the exploration I saw that the overlap in years of the datasets was 1977-2016, so this is what I narrowed both datasets down to. Another step in the initial surface level cleaning was to get rid of columns that were not useful to our project. For example one of the columns I got rid of from the Alcohol consumption dataset was the state abbreviations because there was no state abbreviation column in the crime dataset, so that column was practically useless. I also got rid of multiple columns in the crime dataset since our project will be more focused on violent crime. I deleted the subcategories for property crime but still kept the totals because the insights could be used for deeper analytics in our project. 
	Since we got delayed a little bit with some unforeseen circumstances, we have an updated timeline. Since we were able to build out our file structure and write descriptions on how to access the data as well as the initial combining and cleaning of our data, we can move onto the exploration analysis next. Within the next week, we will complete basic exploration analysis to get a basis on what the data is holding. Once this is further completed we will go deeper into the data cleaning and integration. The hope is to successfully combine our two datasets since they have been made compatible so that one dataset contains both data of alcohol consumption and crime stats per state per year from 1977-2016. After that, we will work on making predictive models. We will then create scripts to automate the processes of acquisition, cleaning, and modeling so that it can be reproduced in the future. We will then work to finalize the markdown reports in which we will look at the FAIR principles giving a description of our ratings. We will finally end it with a final check and release of our project before the final project due date.
